{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: in terminal ulimit -n 4096 and then open this notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from pathlib import Path\n",
    "from quickumls import QuickUMLS\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_files (path_file):\n",
    "    \"\"\"\n",
    "    param: path to file\n",
    "    returns the text in the file as a string \n",
    "    \"\"\"\n",
    "    text_list = []\n",
    "    with open(path_file, encoding='ISO-8859-1') as infile:\n",
    "            for row in infile:\n",
    "                row = row.strip('\\n')\n",
    "                row = row.strip(\"\\\\\")\n",
    "                text_list.append(row)\n",
    "                joined_strings = ','.join(text_list)\n",
    "            \n",
    "    return joined_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_strings_to_qumls (path_file, qumls_file): \n",
    "    \"\"\"\n",
    "    param: path to file\n",
    "    param2: path to directory where qumls files are stored \n",
    "    returns of a file all entities which are mapped by quickumls to the KG\n",
    "    \"\"\"\n",
    "    joined_strings = reading_files (path_file)\n",
    "    matcher = QuickUMLS(qumls_file)\n",
    "    maps = matcher.match(joined_strings, best_match=True, ignore_syntax=False)\n",
    "    \n",
    "    return maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itterating_sentences (file):\n",
    "    \"\"\"\n",
    "    param: path to file \n",
    "    returns list of sentences\n",
    "    \"\"\"\n",
    "    columns = []\n",
    "    sentences = []\n",
    "    with open(file, encoding='ISO-8859-1') as csv_file:\n",
    "        for row in csv_file:\n",
    "            row = row.strip('\\n') \n",
    "            column = row.split('\\t')\n",
    "            columns.append(column)\n",
    "    for x in columns[1:]: # leaving out headers\n",
    "        sentences.append(x[0])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_maps_are_IC_eng(path_file, condition, qumls_file_eng):\n",
    "    \"\"\"\n",
    "    param: path to file\n",
    "    param2: list of strings with HIV indicors\n",
    "    param3: path to directory where english qumls files are stored\n",
    "    returns list of dictionaries with key position of word in file and value file name, the concept and similarity score\n",
    "    \"\"\"\n",
    "    maps_eng = map_strings_to_qumls (path_file, qumls_file_eng)\n",
    "    dict_list = []\n",
    "    for qumls_list in maps_eng:\n",
    "        for qumls_dict in qumls_list:\n",
    "            new_dict = {}\n",
    "            sim_score = qumls_dict[\"similarity\"]\n",
    "            good_term = qumls_dict[\"term\"]\n",
    "            position = qumls_dict[\"end\"]\n",
    "            for ent in condition:\n",
    "                if good_term.lower() == ent.lower():\n",
    "                    new_dict[position] = os.path.basename(path_file), good_term, sim_score\n",
    "                    dict_list.append(new_dict) \n",
    "    return dict_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def matching_strings (search_list, text): \n",
    "    \"\"\"\n",
    "    param1: list with regular expressions\n",
    "    param2: where regular expressions should be found\n",
    "    returns list with items which are found\n",
    "    \n",
    "    \"\"\"\n",
    "    all_list = []\n",
    "    for item in search_list:\n",
    "        try:\n",
    "            if re.findall(item, text, re.IGNORECASE) == None:\n",
    "                continue\n",
    "            if re.findall(item, text, re.IGNORECASE) == []:\n",
    "                continue\n",
    "            else:\n",
    "                found = re.findall(item, text, re.IGNORECASE)\n",
    "                all_list.append(found)\n",
    "              \n",
    "        except:\n",
    "            continue\n",
    "    return all_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_if_maps_are_IC (path_file, condition, combined = \"yes\"):\n",
    "    \"\"\"\n",
    "    param: path to file\n",
    "    param2: keyargument are the IC's, if you would like to check e.g. medications, you could change this condition argument. NOTE: this should be of the format: list of list of strings, where each list of strings is one entity\n",
    "    param3: keyargument is combination of Dutch and English UMLS, \"nl\" means only Dutch, \"en\" means only English data base\n",
    "    returns a set if the file contains an IC and therefore the patient should be recommended testing\n",
    "    \"\"\"\n",
    "    maps = map_strings_to_qumls(path_file, qumls_file)\n",
    "\n",
    "    dict_list = []\n",
    "    if combined ==\"yes\":\n",
    "            for qumls_list in maps:\n",
    "                for qumls_dict in qumls_list:\n",
    "                    new_dict = {}\n",
    "                    sim_score = qumls_dict[\"similarity\"]\n",
    "                    good_term = qumls_dict[\"term\"]\n",
    "                    position = qumls_dict[\"end\"]\n",
    "                    for ent in condition:\n",
    "                        if good_term.lower() == ent.lower():\n",
    "                            new_dict[position] = os.path.basename(path_file), good_term, sim_score\n",
    "                            dict_list.append(new_dict)\n",
    "\n",
    "\n",
    "            for dicto in  check_if_maps_are_IC_eng(path_file, condition, qumls_file_eng):\n",
    "                dict_list.append(dicto) if  dicto  not in dict_list else dict_list\n",
    "    return dict_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_if_maps_are_IC (path_file, condition, combined = \"yes\"):\n",
    "    \"\"\"\n",
    "    param: path to file\n",
    "    param2: keyargument are the IC's, if you would like to check e.g. medications, you could change this condition argument. NOTE: this should be of the format: list of list of strings, where each list of strings is one entity\n",
    "    param3: keyargument is combination of Dutch and English UMLS, \"nl\" means only Dutch, \"en\" means only English data base\n",
    "    returns a set if the file contains an IC and therefore the patient should be recommended testing\n",
    "    \"\"\"\n",
    "    maps = map_strings_to_qumls(path_file, qumls_file)\n",
    "\n",
    "    dict_list = []\n",
    "    if combined == \"yes\":\n",
    "        for qumls_list in maps:\n",
    "            for qumls_dict in qumls_list:\n",
    "                new_dict = {}\n",
    "                sim_score = qumls_dict[\"similarity\"]\n",
    "                good_term = qumls_dict[\"term\"]\n",
    "                position = qumls_dict[\"end\"]\n",
    "                for ent in condition:\n",
    "                    if good_term.lower() == ent.lower():\n",
    "                        new_dict[position] = os.path.basename(path_file), good_term, sim_score\n",
    "                        dict_list.append(new_dict)\n",
    "\n",
    "\n",
    "        for dicto in  check_if_maps_are_IC_eng(path_file, condition, qumls_file_eng):\n",
    "            dict_list.append(dicto) if  dicto  not in dict_list else dict_list\n",
    "        \n",
    "        item = reg_ex_list     \n",
    "        with open (path_file, encoding='ISO-8859-1') as infile:\n",
    "            text = infile.read()\n",
    "            x = matching_strings (item, text)\n",
    "            for listo in x:\n",
    "                new_dict={}\n",
    "                new_dict['reg ex'] = os.path.basename(path_file), listo[0], 1\n",
    "                dict_list.append(new_dict)\n",
    "        \n",
    "    if combined ==\"nlen\":\n",
    "        for qumls_list in maps:\n",
    "            for qumls_dict in qumls_list:\n",
    "                new_dict = {}\n",
    "                sim_score = qumls_dict[\"similarity\"]\n",
    "                good_term = qumls_dict[\"term\"]\n",
    "                position = qumls_dict[\"end\"]\n",
    "                for ent in condition:\n",
    "                    if good_term.lower() == ent.lower():\n",
    "                        new_dict[position] = os.path.basename(path_file), good_term, sim_score\n",
    "                        dict_list.append(new_dict)\n",
    "\n",
    "\n",
    "        for dicto in  check_if_maps_are_IC_eng(path_file, condition, qumls_file_eng):\n",
    "            dict_list.append(dicto) if  dicto  not in dict_list else dict_list\n",
    "        \n",
    "     \n",
    "    if combined == \"nl\":\n",
    "        for qumls_list in maps:\n",
    "            for qumls_dict in qumls_list:\n",
    "                new_dict = {}\n",
    "                sim_score = qumls_dict[\"similarity\"]\n",
    "                good_term = qumls_dict[\"term\"]\n",
    "                position = qumls_dict[\"end\"]\n",
    "                for ent in condition:\n",
    "                    if good_term.lower() == ent.lower():\n",
    "                        new_dict[position] = os.path.basename(path_file), good_term, sim_score\n",
    "                        dict_list.append(new_dict)\n",
    "                        \n",
    "    if combined == \"en\":\n",
    "        for qumls_list in map_strings_to_qumls (path_file, qumls_file_eng):\n",
    "            for qumls_dict in qumls_list:\n",
    "                new_dict = {}\n",
    "                sim_score = qumls_dict[\"similarity\"]\n",
    "                good_term = qumls_dict[\"term\"]\n",
    "                position = qumls_dict[\"end\"]\n",
    "                for ent in condition:\n",
    "                    if good_term.lower() == ent.lower():\n",
    "                        new_dict[position] = os.path.basename(path_file), good_term, sim_score\n",
    "                        dict_list.append(new_dict)\n",
    "        \n",
    "    return dict_list\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itterating_folder (path_to_directory):\n",
    "    \"\"\"\n",
    "    param: path to directory where the files are stored\n",
    "    returns list with all the file names in a directory as strings\n",
    "    \"\"\"\n",
    "    pathlist = []\n",
    "    for path in Path(path_to_directory).glob(\"*\"):\n",
    "         path_in_str = str(path)\n",
    "         pathlist.append(path_in_str)\n",
    "    return pathlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_whole_directory_qulms_ic(path_to_directory, combined=\"yes\"): \n",
    "    \"\"\"\n",
    "    param: path to directory where the files are stored\n",
    "    returns a list of which files contain an IC and therefore the patient should be recommended testing\n",
    "    \"\"\"\n",
    "    recommendation_list = []\n",
    "    for path_file in itterating_folder (path_to_directory):\n",
    "        recommendation = check_if_maps_are_IC (path_file, condition, combined)\n",
    "        recommendation_list.append(recommendation)\n",
    "        recommendation_list2 = [x for x in recommendation_list if x != set()]\n",
    "    return recommendation_list2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def find_highest_sim_score(path_to_directory, combined=\"yes\"):\n",
    "    \"\"\"\n",
    "    param: path to directory where the files are stored\n",
    "    returns list with dictionaries key: position of term in text and value: number, concept, similarity score with term\n",
    "    \"\"\"\n",
    "    merge_same_key_dict = defaultdict(list)\n",
    "    for one_list in check_whole_directory_qulms_ic(path_to_directory, combined=\"yes\"):\n",
    "        for one_dict in one_list: \n",
    "            for key, value in one_dict.items():\n",
    "                merge_same_key_dict[key].append(value)\n",
    "\n",
    "\n",
    "    dict_list = []  \n",
    "    for key, value in merge_same_key_dict.items():\n",
    "        dicta = {}\n",
    "        if len(value) == 1:\n",
    "            dicta[key] = value  \n",
    "            dict_list.append(dicta)\n",
    "        if len(value) >1:\n",
    "            listo = []\n",
    "            for tup in value:\n",
    "                listo.append(tup[2])\n",
    "                highest = max(listo)\n",
    "                if highest == tup[2]:\n",
    "                    new = (tup[0], tup[1], highest)\n",
    "                    dicta[key] = [new]\n",
    "                    dict_list.append(dicta)\n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "def create_xls_file(path_to_directory, combined=\"yes\"):\n",
    "    \"\"\"\n",
    "    param: path to directory \n",
    "    param2: keyword argument if the versions should be combined, \"yes\" means combined (Dutch, English, Regex), \"nlen\" means without regex, \"nl\" is only Dutch and \"en\" is only English \n",
    "    returns dataframe with name of note, concept and similarity score \n",
    "    \"\"\"\n",
    "    list_of_dicts = find_highest_sim_score(path_to_directory, combined=\"yes\")\n",
    "    tup_list = []\n",
    "    for dicto in list_of_dicts:\n",
    "        for key, value in dicto.items():\n",
    "            for tup in value:\n",
    "                tup_list.append(tup)\n",
    "    df = pd.DataFrame(tup_list, columns =['Note', 'Concept', 'Sim Score']) \n",
    "    return df\n",
    "  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_scores(df1, df2) : \n",
    "    \"\"\"\n",
    "    param1: pandas dataframe with name of note, concept and similarity score \n",
    "    param2: pandas dataframe with name of note, concept and similarity score  which should be evaluated against df1\n",
    "    returns list with dicionaries with counters of true positives, false negatives and false positives \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    x = df1[\"Note\"].unique()\n",
    "    x = list(x)\n",
    "   \n",
    "    eval_dict_list = []\n",
    "   \n",
    "    for el in x:\n",
    "        TP_counter = 0 \n",
    "        FN_counter = 0\n",
    "        eval_dict = {}\n",
    "        note = df1.loc[df1.Note == el][\"Concept\"]\n",
    "        note_man = df2.loc[df2.Note == el][\"Concept\"]\n",
    "        note = list(note)\n",
    "        note_man=list(note_man)\n",
    "        new_list = []\n",
    "        new_list2 = []\n",
    "        dict_list = []\n",
    "        for el in note_man:\n",
    "            new_list.append(el.lower())\n",
    "        for el in note:\n",
    "            new_list2.append(el.lower())\n",
    "\n",
    "        for el in new_list:\n",
    "            if el in new_list2:\n",
    "                TP_counter += 1\n",
    "                new_list2.remove(el)\n",
    "            else:\n",
    "                FN_counter +=1\n",
    "        eval_dict[\"TP\"] = TP_counter\n",
    "        eval_dict[\"FN\"] = FN_counter\n",
    "        eval_dict[\"FP\"] = len(new_list2)\n",
    "        eval_dict_list.append(eval_dict)\n",
    "        \n",
    "    return  eval_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def summing_eval_values(df1, df2):\n",
    "    \"\"\"\n",
    "    param1: pandas dataframe with name of note, concept and similarity score \n",
    "    param2: pandas dataframe with name of note, concept and similarity score  which should be evaluated against df1\n",
    "    returs list with combined dictonaries \n",
    "    \"\"\"\n",
    "    score_list = []\n",
    "    evaluation_scores(df1, df2) \n",
    "\n",
    "    dd = defaultdict(list)\n",
    "\n",
    "    for d in (evaluation_scores(df1, df2)):\n",
    "        for key, value in d.items():\n",
    "            dd[key].append(value)\n",
    "    for key, value in dd.items():\n",
    "          score_list.append(key + \" \" + str(sum(value)))\n",
    "    return score_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtaining_recall_precision_f_score(df1, df2): \n",
    "    \"\"\"\n",
    "    param1: pandas dataframe with name of note, concept and similarity score \n",
    "    param2: pandas dataframe with name of note, concept and similarity score  which should be evaluated against df1\n",
    "    returns dictionary with evaluation scores \n",
    "    \"\"\"\n",
    "    \n",
    "    eval_dict = {}\n",
    "\n",
    "    TP = summing_eval_values(df1, df2)[0]\n",
    "    integer_TP =int(TP[2:])\n",
    "    FN = summing_eval_values(df1, df2)[1]\n",
    "    integer_FN =int(FN[2:])\n",
    "    FP = summing_eval_values(df1, df2)[2]\n",
    "    integer_FP =int(FP[2:])\n",
    "    \n",
    "    precision = ((integer_TP)/(integer_TP+integer_FP))\n",
    "    recall = ((integer_TP)/(integer_TP+integer_FN))\n",
    "    f_score = 2*((precision*recall)/(precision+recall))\n",
    "    eval_dict[\"recall\"] = recall\n",
    "    eval_dict[\"precision\"] = precision\n",
    "    eval_dict[\"f_score\"] = f_score\n",
    "    \n",
    "    return eval_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.7894736842105263,\n",
       " 'precision': 0.8426966292134831,\n",
       " 'f_score': 0.8152173913043478}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill in paths: \n",
    "from Creating_rex_ex_list import creating_reg_ex_list\n",
    "\n",
    "# path to Dutch qumls files\n",
    "qumls_file ='/Users/lumei/Downloads/dut' \n",
    "# path to English qumls files\n",
    "qumls_file_eng='/Users/lumei/Downloads/eng'\n",
    "#Path to test notes (text files)\n",
    "path_to_directory = \"/Users/lumei/Desktop/Annotated_Notes\"\n",
    "#Path to file where conditions are stored\n",
    "condition_file = \"/Users/lumei/Documents/Stage/IC_list.csv\"\n",
    "condition = itterating_sentences(condition_file)\n",
    "reg_ex_list= creating_reg_ex_list(condition_file, qumls_file, qumls_file_eng)\n",
    "df1 = create_xls_file(path_to_directory)\n",
    "#Path to manual annotations file \n",
    "path_to_manual_annotations = \"/Users/lumei/Desktop/Mannual Annotations.csv\"\n",
    "df2 =  pd.read_excel(path_to_manual_annotations) \n",
    "\n",
    "obtaining_recall_precision_f_score(df1, df2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
